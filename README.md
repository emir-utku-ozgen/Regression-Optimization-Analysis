# Regression & Optimization Algorithm Analysis ğŸš€

This project focuses on analyzing the performance and weight convergence trajectories ($w$) of various optimization algorithms on high-dimensional NLP data.

## ğŸ“ Project Overview
A binary classification regression model was trained using synthetic text data generated by LLMs. The primary objective is to compare the efficiency and behavior of optimization algorithms within the loss landscape.

### ğŸ›  Technical Architecture
- **Data Generation:** 200 synthetic QA pairs generated via `Turkish-Gemma-9b-T1`.
- **Vector Representation:** 768-dimensional semantic embeddings generated using the `turkish-e5-large` model.
- **Model Function:** $y = \tanh(w \cdot x)$
- **Analysis Method:** 2D projection of high-dimensional weight updates ($w_{1:t}$) using T-SNE.

## ğŸ“Š Comparison of Optimization Algorithms
The following algorithms were tested using 5 different random initial weights ($w_0$):

* **Gradient Descent (GD):** Deterministic updates performed over the entire dataset.
* **Stochastic Gradient Descent (SGD):** Fast but noisy gradient updates.
* **Adam Optimizer:** Dynamic convergence using momentum and adaptive learning rates.

### ğŸ“‰ Visualization & Results (T-SNE)
The optimization paths taken by each algorithm are reduced to 2 dimensions using the T-SNE algorithm and presented as trajectories in the weight space:

![T-SNE Optimization Trajectories](output/tsne_output.png)
*Figure: Convergence trajectories of GD, SGD, and Adam algorithms in the weight space.*

## ğŸ“‚ Project Structure
- `Regression-Optimization-Analysis.ipynb`: Main notebook containing data prep, training, and visualization.
- `/data/`: Includes `raw/` (text data) and `processed/` (vector embeddings).
- `/output/`: Loss plots, accuracy scores, and T-SNE outputs.
- `rapor.pdf`: Technical report covering the mathematical background and findings.

---
*Developed as part of the Computer Engineering curriculum at YÄ±ldÄ±z Technical University.*
